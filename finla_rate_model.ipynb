{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finla rate model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be1qoG_t5TuV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "045bbeb5-f9e4-4995-8a8f-0cf09b9ae9fe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBLaGvlLT72-"
      },
      "source": [
        "def save_obj(obj, name ):\n",
        "    with open('/content/drive/My Drive/'+ name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe85kh91UH3G"
      },
      "source": [
        "save_obj(word_to_index, 'word_to_index' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKOIsLxE5a9N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "55eae7c4-e0ca-464f-b1f9-77b046a07be1"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words(\"english\")) - {'no' , 'not', 'very'}\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKQ3bnPzdW1i"
      },
      "source": [
        "#functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eLrpFfW5gEJ"
      },
      "source": [
        "def cleaned_text_method2(text):\n",
        "\n",
        "    cleaned_text = re.sub('http.*','',text)\n",
        "\n",
        "    # specific\n",
        "    cleaned_text = re.sub(r\"won\\'t\", \"will not\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"can\\'t\", \"can not\", cleaned_text)\n",
        "\n",
        "    # general\n",
        "    cleaned_text = re.sub(r\"n\\'t\", \" not\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'re\", \" are\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'s\", \" is\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'d\", \" would\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'ll\", \" will\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'t\", \" not\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'ve\", \" have\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'m\", \" am\", cleaned_text)\n",
        "    \n",
        "    cleaned_text = cleaned_text.lower()\n",
        "\n",
        "    cleaned_text = \" \".join([w for w in cleaned_text.split() if not w in stops])\n",
        "    \n",
        "    cleaned_text = re.sub(\"[^a-zA-Z]\", \" \",cleaned_text)\n",
        "\n",
        "    #cleaned_text = \" \".join([w for w in cleaned_text.split() if len(w) > 1])\n",
        "\n",
        "    cleaned_text = re.sub('\\s+',' ',cleaned_text)\n",
        "                              \n",
        "    return cleaned_text "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5vGT3Rt5raj"
      },
      "source": [
        "def sentences_to_indices(X, word_to_index, maxLen):\n",
        "\n",
        "  m = X.shape[0] # number of training examples\n",
        "    \n",
        "  X_indices = np.zeros((m, maxLen),dtype=int)\n",
        "  i = 0\n",
        "  for s in X:\n",
        "\n",
        "    sentence_words = s.split()\n",
        "    \n",
        "    j = 0\n",
        "    \n",
        "    for w in sentence_words:\n",
        "      if j >= maxLen:\n",
        "        break\n",
        "      if w in word_to_index:\n",
        "        X_indices[i, j] = int(word_to_index[w])\n",
        "      j = j + 1\n",
        "    i = i + 1\n",
        "        \n",
        "  return X_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9Vod1PCEBcW"
      },
      "source": [
        "def create_mpdel(NB_WORDS, GLOVE_DIM, MAX_LEN, weights_file_path):\n",
        "  model=Sequential()\n",
        "  model.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
        "  model.add(LSTM(128 , return_sequences=True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(128 , return_sequences=False))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dense(5, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.load_weights(weights_file_path)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYGnTddzUvRR"
      },
      "source": [
        "def load_obj(word_to_index_path):\n",
        "    with open(word_to_index_path, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW_kULcTcieQ"
      },
      "source": [
        "def init(word_to_index_path, weights_file_path):\n",
        "  word_to_index = load_obj(word_to_index_path)\n",
        "\n",
        "  NB_WORDS = len(word_to_index)+1\n",
        "  MAX_LEN = 99\n",
        "  GLOVE_DIM = 50\n",
        "\n",
        "  model = create_mpdel(NB_WORDS, GLOVE_DIM, MAX_LEN, weights_file_path)\n",
        "  return model, word_to_index, MAX_LEN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VTfCWr8H0fd"
      },
      "source": [
        "def predict(x_test, model, word_to_index, MAX_LEN):\n",
        "  for i in range(len(x_test)):\n",
        "    x_test[i] = cleaned_text_method2(x_test[i])\n",
        "  X_test_indices = sentences_to_indices(x_test, word_to_index, MAX_LEN)\n",
        "  y_pred = model.predict(X_test_indices)\n",
        "\n",
        "  y_pred[:,3] = y_pred[:,3] + 0.02\n",
        "  y_pred[:,4] = y_pred[:,4] - 0.02\n",
        "  y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "  y_pred_bool = y_pred_bool + 1\n",
        "\n",
        "  try:\n",
        "    reate1_comment, = x_test[np.where(np.isclose(y_pred[:,0] , y_pred[y_pred_bool == 1][:,0].max()))[0]]\n",
        "  except:\n",
        "    reate1_comment = \"\"\n",
        "  try:\n",
        "    reate2_comment, =  x_test[np.where(np.isclose(y_pred[:,1] , y_pred[y_pred_bool == 2][:,1].max()))[0]]\n",
        "  except:\n",
        "    reate2_comment = \"\"\n",
        "  try:\n",
        "    reate3_comment, =  x_test[np.where(np.isclose(y_pred[:,2] , y_pred[y_pred_bool == 3][:,2].max()))[0]]\n",
        "  except:\n",
        "    reate3_comment = \"\"\n",
        "  try:\n",
        "    reate4_comment, =  x_test[np.where(np.isclose(y_pred[:,3] , y_pred[y_pred_bool == 4][:,3].max()))[0]]\n",
        "  except:\n",
        "    reate4_comment = \"\"\n",
        "  try:\n",
        "    reate5_comment, =  x_test[np.where(np.isclose(y_pred[:,4] , y_pred[y_pred_bool == 5][:,4].max()))[0]]\n",
        "  except:\n",
        "    reate5_comment = \"\"\n",
        "\n",
        "  return y_pred_bool, reate1_comment, reate2_comment, reate3_comment, reate4_comment, reate5_comment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN0Rz1JldcSk"
      },
      "source": [
        "initializing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hamBmY8Gc-FJ"
      },
      "source": [
        "word_to_index_path = '/content/drive/My Drive/word_to_index.pkl'\n",
        "weights_file_path = '/content/drive/My Drive/model3.h5'\n",
        "\n",
        "model, word_to_index, MAX_LEN = init(word_to_index_path, weights_file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwA9q2WIdyai"
      },
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB3R4A176BSL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fb77c897-4c19-4084-a75b-a6e9351199cd"
      },
      "source": [
        "x_test = np.array([\"You were responsive to my urgent shipping needs,Thank you.\",\n",
        "                   \"This prouduct is too expensive and i don't like the price\",\n",
        "                   \"The Movie was extremly good but a lot of things was not completed\",\n",
        "                   \"I wanted to let you know that I am quite pleased with you fast delivery & your product , which works well. As advertised\",\n",
        "                   \"As per the subject - your customer service is awesome \",\n",
        "                   \"your website is awesome as are you prices - sold - you have a new customer! THANKS\",\n",
        "                   \"The Services Was not good enough after selling. Thank You\",\n",
        "                   \"This goes to the wonderful, very helpful, and very knowledgeable team of people\",\n",
        "                   \"Love this t-shirt it has a good price too THANKSSS\",\n",
        "                   \"Beatuiful movie i like it very much, GOOD TEAM\",\n",
        "                   \"The customar services is not effective please take care of the customers\",\n",
        "                   \"the materials isn't good enough so the products broke from the second time from the begining of use\",\n",
        "                   \"This actions involves racism so it's bad habbits\",\n",
        "                   \"i think it is good enough to complete it\",\n",
        "                   \"it is very good\"])\n",
        "\n",
        "predictions = predict(x_test, model, word_to_index, MAX_LEN)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([5, 2, 3, 5, 5, 5, 1, 5, 5, 5, 1, 2, 1, 4, 4]), 'actions involves racism bad habbits', 'materials not good enough products broke second time begining use', 'movie extremly good lot things not completed', 'think good enough complete', 'website awesome prices sold new customer thanks')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBkT8dmLdzsa"
      },
      "source": [
        "#OOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozUqcjjXOMxY"
      },
      "source": [
        "class CommentRater:\n",
        "\n",
        "  def __init__(self, word_to_index_path, weights_file_path):\n",
        "    self.word_to_index = self.load_obj(word_to_index_path)\n",
        "    self.NB_WORDS = len(self.word_to_index)+1\n",
        "    self.MAX_LEN = 99\n",
        "    self.GLOVE_DIM = 50\n",
        "    self.model = self.create_mpdel(self.NB_WORDS, self.GLOVE_DIM, self.MAX_LEN, weights_file_path)\n",
        "\n",
        "\n",
        "\n",
        "  def cleaned_text_method2(self, text):\n",
        "    cleaned_text = re.sub('http.*','',text)\n",
        "    # specific\n",
        "    cleaned_text = re.sub(r\"won\\'t\", \"will not\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"can\\'t\", \"can not\", cleaned_text)\n",
        "    # general\n",
        "    cleaned_text = re.sub(r\"n\\'t\", \" not\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'re\", \" are\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'s\", \" is\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'d\", \" would\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'ll\", \" will\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'t\", \" not\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'ve\", \" have\", cleaned_text)\n",
        "    cleaned_text = re.sub(r\"\\'m\", \" am\", cleaned_text)\n",
        "    \n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    cleaned_text = \" \".join([w for w in cleaned_text.split() if not w in stops])\n",
        "    cleaned_text = re.sub(\"[^a-zA-Z]\", \" \",cleaned_text)\n",
        "    #cleaned_text = \" \".join([w for w in cleaned_text.split() if len(w) > 1])\n",
        "    cleaned_text = re.sub('\\s+',' ',cleaned_text)\n",
        "                              \n",
        "    return cleaned_text \n",
        "\n",
        "\n",
        "\n",
        "  def sentences_to_indices(self, X, word_to_index, maxLen):\n",
        "    m = X.shape[0] # number of training examples\n",
        "    X_indices = np.zeros((m, maxLen),dtype=int)\n",
        "    i = 0\n",
        "    for s in X:\n",
        "      sentence_words = s.split()\n",
        "      j = 0\n",
        "      for w in sentence_words:\n",
        "        if j >= maxLen:\n",
        "          break\n",
        "        if w in word_to_index:\n",
        "          X_indices[i, j] = int(word_to_index[w])\n",
        "        j = j + 1\n",
        "      i = i + 1\n",
        "          \n",
        "    return X_indices\n",
        "\n",
        "\n",
        "\n",
        "  def create_mpdel(self, NB_WORDS, GLOVE_DIM, MAX_LEN, weights_file_path):\n",
        "    model=Sequential()\n",
        "    model.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
        "    model.add(LSTM(128 , return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(128 , return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.load_weights(weights_file_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "  def load_obj(self, word_to_index_path):\n",
        "      with open(word_to_index_path, 'rb') as f:\n",
        "          return pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "  def predict(self, x_test):\n",
        "    for i in range(len(x_test)):\n",
        "      x_test[i] = self.cleaned_text_method2(x_test[i])\n",
        "    X_test_indices = self.sentences_to_indices(x_test, self.word_to_index, self.MAX_LEN)\n",
        "    y_pred = self.model.predict(X_test_indices)\n",
        "    y_pred[:,3] = y_pred[:,3] + 0.02\n",
        "    y_pred[:,4] = y_pred[:,4] - 0.02\n",
        "    y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "    y_pred_bool = y_pred_bool + 1\n",
        "\n",
        "    try:\n",
        "      reate1_comment, = x_test[np.where(np.isclose(y_pred[:,0] , y_pred[y_pred_bool == 1][:,0].max()))[0]]\n",
        "    except:\n",
        "      reate1_comment = \"\"\n",
        "    try:\n",
        "      reate2_comment, =  x_test[np.where(np.isclose(y_pred[:,1] , y_pred[y_pred_bool == 2][:,1].max()))[0]]\n",
        "    except:\n",
        "      reate2_comment = \"\"\n",
        "    try:\n",
        "      reate3_comment, =  x_test[np.where(np.isclose(y_pred[:,2] , y_pred[y_pred_bool == 3][:,2].max()))[0]]\n",
        "    except:\n",
        "      reate3_comment = \"\"\n",
        "    try:\n",
        "      reate4_comment, =  x_test[np.where(np.isclose(y_pred[:,3] , y_pred[y_pred_bool == 4][:,3].max()))[0]]\n",
        "    except:\n",
        "      reate4_comment = \"\"\n",
        "    try:\n",
        "      reate5_comment, =  x_test[np.where(np.isclose(y_pred[:,4] , y_pred[y_pred_bool == 5][:,4].max()))[0]]\n",
        "    except:\n",
        "      reate5_comment = \"\"\n",
        "\n",
        "    return y_pred_bool, reate1_comment, reate2_comment, reate3_comment, reate4_comment, reate5_comment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z89-J1GfJUXF"
      },
      "source": [
        "word_to_index_path = '/content/drive/My Drive/word_to_index.pkl'\n",
        "weights_file_path = '/content/drive/My Drive/model3.h5'\n",
        "\n",
        "model_rater = CommentRater(word_to_index_path, weights_file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p56SIDHWX7Vf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "011fd1f4-d685-4a6e-ddf0-2c9843b47dbe"
      },
      "source": [
        "comments = [\"You were responsive to my urgent shipping needs,Thank you.\",\n",
        "                   \"This prouduct is too expensive and i don't like the price\",\n",
        "                   \"The Movie was extremly good but a lot of things was not completed\",\n",
        "                   \"I wanted to let you know that I am quite pleased with you fast delivery & your product , which works well. As advertised\",\n",
        "                   \"As per the subject - your customer service is awesome \",\n",
        "                   \"your website is awesome as are you prices - sold - you have a new customer! THANKS\",\n",
        "                   \"The Services Was not good enough after selling. Thank You\",\n",
        "                   \"This goes to the wonderful, very helpful, and very knowledgeable team of people\",\n",
        "                   \"Love this t-shirt it has a good price too THANKSSS\",\n",
        "                   \"Beatuiful movie i like it very much, GOOD TEAM\",\n",
        "                   \"The customar services is not effective please take care of the customers\",\n",
        "                   \"the materials isn't good enough so the products broke from the second time from the begining of use\",\n",
        "                   \"This actions involves racism so it's bad habbits\",\n",
        "                   \"i think it is good enough to complete it\",\n",
        "                   \"it is very good\"]\n",
        "x_test = np.array(comments)\n",
        "y_predict, reate1_comment, reate2_comment, reate3_comment, reate4_comment, reate5_comment = model_rater.predict(x_test)\n",
        "for i in range(len(x_test)):\n",
        "  print('comment : ' , comments[i])\n",
        "  print('rate    : ' , y_predict[i])\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comment :  You were responsive to my urgent shipping needs,Thank you.\n",
            "rate    :  5\n",
            "\n",
            "comment :  This prouduct is too expensive and i don't like the price\n",
            "rate    :  2\n",
            "\n",
            "comment :  The Movie was extremly good but a lot of things was not completed\n",
            "rate    :  3\n",
            "\n",
            "comment :  I wanted to let you know that I am quite pleased with you fast delivery & your product , which works well. As advertised\n",
            "rate    :  5\n",
            "\n",
            "comment :  As per the subject - your customer service is awesome \n",
            "rate    :  5\n",
            "\n",
            "comment :  your website is awesome as are you prices - sold - you have a new customer! THANKS\n",
            "rate    :  5\n",
            "\n",
            "comment :  The Services Was not good enough after selling. Thank You\n",
            "rate    :  1\n",
            "\n",
            "comment :  This goes to the wonderful, very helpful, and very knowledgeable team of people\n",
            "rate    :  5\n",
            "\n",
            "comment :  Love this t-shirt it has a good price too THANKSSS\n",
            "rate    :  5\n",
            "\n",
            "comment :  Beatuiful movie i like it very much, GOOD TEAM\n",
            "rate    :  5\n",
            "\n",
            "comment :  The customar services is not effective please take care of the customers\n",
            "rate    :  1\n",
            "\n",
            "comment :  the materials isn't good enough so the products broke from the second time from the begining of use\n",
            "rate    :  2\n",
            "\n",
            "comment :  This actions involves racism so it's bad habbits\n",
            "rate    :  1\n",
            "\n",
            "comment :  i think it is good enough to complete it\n",
            "rate    :  4\n",
            "\n",
            "comment :  it is very good\n",
            "rate    :  5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNxs1gG-YV80"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}